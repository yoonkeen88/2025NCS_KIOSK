# -*- coding: utf-8 -*-
"""DataCollection01.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lSFvIHy7TEBjcbpX7Hf2WOMtYxrDq6QK
"""

# 웹 스크레이핑 (웹 크롤링): 웹사이트에서 직접 텍스트 데이터를 수집하는 방법

# RSS 피드 이용: RSS는 웹사이트의 업데이트를 XML 형식으로 제공하는 서비스

# OCR(Optical Character Recognition): 이미지나 PDF 등에서 텍스트를 추출

# 개인화된 데이터(전자메일, 소셜미디어, 채팅 기록 등): 사용자 동의 하에 개인화된 텍스트 데이터도 유용한 자료가 됨

# 기업/기관 DB 및 로그 파일: 기업이나 기관에서 보유하고 있는 DB나 로그 파일에도 유용한 텍스트 정보가 많을 수 있음.

# API 이용: Twitter, Reddit 등의 플랫폼에서는 API를 제공하여 사용자가 쉽게 데이터를 수집

# 공개 데이터셋 이용: 이미 공개된 NLP, AI, ML등의 데이터셋을 활용

# import urllib.request

# url = 'https://www.inha.ac.kr/sites/kr/images/logo.png'
# urllib.request.urlretrieve(url, 'inha.png')
# print("saved!")

# import urllib.request

# url = 'https://www.inha.ac.kr/sites/kr/images/logo.png'
# logo = urllib.request.urlopen(url).read()

# with open('inha.png', 'wb') as fp:
#   fp.write(logo)
#   print("saved!")

import urllib.request
import urllib.parse

api = 'https://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp'
id = input('Input local code : ')  # 108, 109, 105
values = {'stnId' : id}
parameters = urllib.parse.urlencode(values)
url = api + '?' + parameters
#print(url)
urls = urllib.request.urlopen(url).read()
texts = urls.decode('utf-8')
#print(texts)

from bs4 import BeautifulSoup

html = """
<html>
<head>
<title>스크레이핑 실습</title>
</head>
<body>
<h1>인하대학교</h1>
<p>웹스크레이핑</p>
<p>넘파이, 판다스, NLP ... </p>
</body>
</html>
"""

soup = BeautifulSoup(html, 'html.parser')
t = soup.html.head.title
h1 = soup.html.body.h1.string
p1 = soup.html.body.p
p2 = p1.next_sibling.next_sibling

print(p1.string)
print(p2.string)
print(h1)
print(t)
print(t.string)